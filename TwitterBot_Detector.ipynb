{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCixRI5ZkzW+tFBfqUI1AJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pekayie/DCIT316/blob/main/TwitterBot_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DEADVczgNl23"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv('/content/drive/MyDrive/bot_detection_data.csv')\n",
        "data.head()\n",
        "#Data Cleaning\n",
        "# Remove duplicate entries\n",
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "JyPHeQ_MNrnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "id": "rkC2TuwnPeXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "#Convert categorical labels to numeric using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "data['Bot Label'] = label_encoder.fit_transform(data['Bot Label'])"
      ],
      "metadata": {
        "id": "JEvgtUQTNwTC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"I am running and eating a better meal\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = sentence.split()\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Join the stemmed words back into a sentence\n",
        "stemmed_sentence = \" \".join(stemmed_words)\n",
        "\n",
        "print(stemmed_sentence)  # Output: \"I am run and eat a better meal\""
      ],
      "metadata": {
        "id": "C0wyauxlUXvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"I am running and eating a better meal\"\n",
        "\n",
        "# Process the sentence using spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Lemmatize each word and join them back into a sentence\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "print(lemmatized_sentence)  # Output: \"I be run and eat a good meal\""
      ],
      "metadata": {
        "id": "UFcKOHkoUmRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca10c02-540f-445f-dae0-241619af36b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I be run and eat a well meal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# Get the list of stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"I am running and eating a better meal\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = sentence.split()\n",
        "\n",
        "# Remove the stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords_list]\n",
        "\n",
        "# Join the filtered words back into a sentence\n",
        "filtered_sentence = \" \".join(filtered_words)\n",
        "\n",
        "print(filtered_sentence)  # Output: \"running eating better meal\""
      ],
      "metadata": {
        "id": "CNiHYcitUreO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec2b7eb-1140-4775-a82f-577914a77b65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running eating better meal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction\n",
        "# Using TF-IDF vectorization for text features\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "text_features = vectorizer.fit_transform(data['Tweet'])"
      ],
      "metadata": {
        "id": "Tr_ioITQN3R5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create processed dataset with cleaned and extracted features\n",
        "processed_data = pd.concat([data['Bot Label'], pd.DataFrame(text_features.toarray())], axis=1)"
      ],
      "metadata": {
        "id": "OeQfNvqpN_9U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the processed dataset into training and testing sets\n",
        "features = processed_data.drop('Bot Label', axis=1)\n",
        "labels = processed_data['Bot Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Ub7j-vVZOFAI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the classification model\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# Fit the model to the training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "xgb_predictions = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "3-9c8kAbOLoe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, xgb_predictions)\n",
        "precision = precision_score(y_test, xgb_predictions)\n",
        "recall = recall_score(y_test, xgb_predictions)\n",
        "f1 = f1_score(y_test, xgb_predictions)"
      ],
      "metadata": {
        "id": "iZ9I2SRLOTlZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the evaluation metrics\n",
        "print(\"XGBoost Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "Vp3qivrqObwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bar Chart Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the performance metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.bar(metrics, values)\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Performance Metrics')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i8nQ5P_cX3QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Roc Curve Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Calculate the predicted probabilities of the positive class\n",
        "probs_positive_class = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the false positive rate and true positive rate\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs_positive_class)\n",
        "\n",
        "# Calculate the area under the ROC curve (AUC)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random model\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxD2umxUX6eQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}